{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset count: 24300 \n",
      "Test dataset count: 2700 \n"
     ]
    }
   ],
   "source": [
    "from data_loader import load_datasets\n",
    "\n",
    "x_train, y_train = load_datasets()\n",
    "x_test,  y_test = load_datasets(train_or_test_folder=\"Test_Alphabet\")\n",
    "\n",
    "print(f\"Train dataset count: {len(x_train)} \\nTest dataset count: {len(x_test)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-Hot Coding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import alphabet_to_num\n",
    "\n",
    "def one_hot_coding(unique_labels, data):\n",
    "    y_oh = torch.zeros(len(data),unique_labels)\n",
    "    for i, class_word in enumerate(data):\n",
    "        index = alphabet_to_num(class_word)\n",
    "        y_oh[i][index] = 1\n",
    "\n",
    "    return y_oh\n",
    "\n",
    "y_train_oh = one_hot_coding(len(np.unique(y_train)),y_train)\n",
    "y_test_oh = one_hot_coding(len(np.unique(y_train)),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 50, 50), (24300,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(3, 5, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(5, 15, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool2): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=135, out_features=27, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=5, padding=2)\n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=15, kernel_size=5, padding=2)\n",
    "        # Another max pooling layer\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(15 * 3 * 3, 27)  # 3x3 is the size after two max pooling layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 15 * 3 * 3)  # Flatten the output of conv2 layer\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = ConvNet()\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21836/1567307064.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_loader = DataLoader(TensorDataset(torch.tensor(x_train),torch.tensor(y_train_oh)),\n",
      "/tmp/ipykernel_21836/1567307064.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_loader = DataLoader(TensorDataset(torch.tensor(x_test),torch.tensor(y_test_oh)),\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(),lr=0.01)\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(x_train),torch.tensor(y_train_oh)), \n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(torch.tensor(x_test),torch.tensor(y_test_oh)), \n",
    "                          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 2.5124\n",
      "Epoch [2/50], Train Loss: 1.4197\n",
      "Epoch [3/50], Train Loss: 1.2344\n",
      "Epoch [4/50], Train Loss: 1.1528\n",
      "Epoch [5/50], Train Loss: 1.1014\n",
      "Epoch [6/50], Train Loss: 1.0685\n",
      "Epoch [7/50], Train Loss: 1.0547\n",
      "Epoch [8/50], Train Loss: 1.0469\n",
      "Epoch [9/50], Train Loss: 1.0182\n",
      "Epoch [10/50], Train Loss: 0.9953\n",
      "Epoch [11/50], Train Loss: 0.9676\n",
      "Epoch [12/50], Train Loss: 0.9148\n",
      "Epoch [13/50], Train Loss: 0.9078\n",
      "Epoch [14/50], Train Loss: 0.8927\n",
      "Epoch [15/50], Train Loss: 0.8863\n",
      "Epoch [16/50], Train Loss: 0.8853\n",
      "Epoch [17/50], Train Loss: 0.8815\n",
      "Epoch [18/50], Train Loss: 0.8656\n",
      "Epoch [19/50], Train Loss: 0.8740\n",
      "Epoch [20/50], Train Loss: 0.9277\n",
      "Epoch [21/50], Train Loss: 0.9316\n",
      "Epoch [22/50], Train Loss: 0.8772\n",
      "Epoch [23/50], Train Loss: 0.8375\n",
      "Epoch [24/50], Train Loss: 0.8123\n",
      "Epoch [25/50], Train Loss: 0.8409\n",
      "Epoch [26/50], Train Loss: 0.7971\n",
      "Epoch [27/50], Train Loss: 0.7840\n",
      "Epoch [28/50], Train Loss: 0.7770\n",
      "Epoch [29/50], Train Loss: 0.7677\n",
      "Epoch [30/50], Train Loss: 0.7523\n",
      "Epoch [31/50], Train Loss: 0.7316\n",
      "Epoch [32/50], Train Loss: 0.7219\n",
      "Epoch [33/50], Train Loss: 0.7179\n",
      "Epoch [34/50], Train Loss: 0.6942\n",
      "Epoch [35/50], Train Loss: 0.7024\n",
      "Epoch [36/50], Train Loss: 0.6894\n",
      "Epoch [37/50], Train Loss: 0.6688\n",
      "Epoch [38/50], Train Loss: 0.6786\n",
      "Epoch [39/50], Train Loss: 0.6718\n",
      "Epoch [40/50], Train Loss: 0.6558\n",
      "Epoch [41/50], Train Loss: 0.7319\n",
      "Epoch [42/50], Train Loss: 0.6543\n",
      "Epoch [43/50], Train Loss: 0.6377\n",
      "Epoch [44/50], Train Loss: 0.6873\n",
      "Epoch [45/50], Train Loss: 0.6412\n",
      "Epoch [46/50], Train Loss: 0.6367\n",
      "Epoch [47/50], Train Loss: 0.6372\n",
      "Epoch [48/50], Train Loss: 0.6377\n",
      "Epoch [49/50], Train Loss: 0.6302\n",
      "Epoch [50/50], Train Loss: 0.6366\n"
     ]
    }
   ],
   "source": [
    "def train_model(model,criterion,optimizer,num_epochs=50):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "train_model(model=model, criterion=criterion, optimizer=optimizer)\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.7930\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels_indices = torch.max(labels, 1)  # Convert one-hot labels to indices\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels_indices).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy on test set: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "test_model(model=model,test_loader=test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
